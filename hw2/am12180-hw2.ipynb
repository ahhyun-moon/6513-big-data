{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e3e2a2-e98c-4ca7-ac7f-95c493065f36",
   "metadata": {},
   "source": [
    "# Big Data HW 2\n",
    "## Ahhyun Moon - am12180@nyu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ba8606-f42e-4849-b358-896bef546bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 00:23:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/conda/envs/bigdata-spark/lib/python3.11/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7b10c40df390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"<my-app-name>\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2b26a4-d970-49cf-a2e6-7774260a7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functinos from Pyspark SQL Library\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0e1db-4254-4035-a658-dca546be9b1b",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25857493-55be-489f-9079-2e06787ae54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare bakery data frame\n",
    "bakeryData = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"./data/Bakery.csv\") \n",
    "# bakeryData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0aea6f-952a-4c2c-b381-fe667e5941f2",
   "metadata": {},
   "source": [
    "#### Showing best-selling items for EVERY Monday (7 - 11 am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a8d595-f1dd-4003-8249-880f8452e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the highest selling item for Mondays, per hour, for the 7AM to 11AM hours. Note that 'weekday', 'period' have to be computed.\n",
      "+---------+--------+-------+----------+-----------+\n",
      "|     item|quantity|weekday|      date|hour-period|\n",
      "+---------+--------+-------+----------+-----------+\n",
      "|    Bread|     165| Monday|2016-10-31|          8|\n",
      "|   Coffee|    1008| Monday|2016-10-31|          9|\n",
      "|   Coffee|    1060| Monday|2016-10-31|         10|\n",
      "|   Coffee|    1587| Monday|2016-10-31|         11|\n",
      "|   Pastry|     739| Monday|2016-11-07|          8|\n",
      "|      Tea|    2237| Monday|2016-11-07|          9|\n",
      "|   Coffee|    5273| Monday|2016-11-07|         10|\n",
      "|   Coffee|    7668| Monday|2016-11-07|         11|\n",
      "|   Coffee|    1275| Monday|2016-11-14|          7|\n",
      "|Medialuna|    2555| Monday|2016-11-14|          8|\n",
      "|   Coffee|    6404| Monday|2016-11-14|          9|\n",
      "|   Coffee|    6459| Monday|2016-11-14|         10|\n",
      "|    Bread|    6493| Monday|2016-11-14|         11|\n",
      "|   Coffee|    1800| Monday|2016-11-21|          7|\n",
      "|   Coffee|    3604| Monday|2016-11-21|          8|\n",
      "|   Coffee|   14463| Monday|2016-11-21|          9|\n",
      "|   Coffee|    7258| Monday|2016-11-21|         10|\n",
      "|   Coffee|    7274| Monday|2016-11-21|         11|\n",
      "|   Pastry|    2218| Monday|2016-11-28|          7|\n",
      "|   Coffee|    2220| Monday|2016-11-28|          8|\n",
      "+---------+--------+-------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update data frame with necessary columns\n",
    "# Add weekday column based on date, \n",
    "# Add hour-period based on time,\n",
    "# Change transaction data type to Int.\n",
    "bakeryData1 = bakeryData.withColumn(\"weekday\", F.date_format(\"date\", \"EEEE\")) \\\n",
    "                        .withColumn(\"hour-period\", F.date_format(\"Time\", \"h\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"transaction\", bakeryData[\"transaction\"].cast(IntegerType()))\n",
    "\n",
    "# Filter transactions between 7 AM and 11:59 AM on Mondays\n",
    "filtered_data = bakeryData1.filter((F.col(\"hour-period\").between(7, 11)) &(F.col(\"weekday\") == \"Monday\"))\n",
    "\n",
    "# Perform the grouping and aggregation for each item and hour\n",
    "grouped_bakeryData1 = filtered_data.groupBy(\"date\",\"weekday\", \"hour-period\", \"item\") \\\n",
    "                                   .agg(F.sum(\"Transaction\").alias(\"quantity\"))\n",
    "\n",
    "# Use window function to get the highest selling item per hour\n",
    "window_spec = Window.partitionBy(\"date\",\"weekday\",\"hour-period\").orderBy(F.desc(\"quantity\"))\n",
    "ranked_data = grouped_bakeryData1.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "\n",
    "# Filter to get only the highest-selling item per hour and order the results by Date and Hour\n",
    "top_selling_items = ranked_data.filter(F.col(\"rank\") == 1).orderBy(\"date\")\n",
    "\n",
    "# Select the required cols and show results\n",
    "print(\"Show the highest selling item for Mondays, per hour, for the 7AM to 11AM hours. Note that 'weekday', 'period' have to be computed.\")\n",
    "top_selling_items.select(\"item\", \"quantity\", \"weekday\", \"date\", \"hour-period\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57d2b4-e8ab-479f-af22-e0c690565363",
   "metadata": {},
   "source": [
    "#### Showing best-selling items for all Mondays (7 - 11 am) \n",
    "Added this solution just in case for different interpretation of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1845af-0312-4708-9634-8d0fb2ae8fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the highest selling item for Mondays, per hour, for the 7AM to 11AM hours. Note that 'weekday', 'period' have to be computed.\n",
      "+------+--------+-------+-----------+\n",
      "|  item|quantity|weekday|hour-period|\n",
      "+------+--------+-------+-----------+\n",
      "|Coffee|    5292| Monday|          7|\n",
      "|Coffee|  129124| Monday|          8|\n",
      "|Coffee|  293503| Monday|          9|\n",
      "|Coffee|  543844| Monday|         10|\n",
      "|Coffee|  523336| Monday|         11|\n",
      "+------+--------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the grouping and aggregation for each item and hour\n",
    "grouped_bakeryData1_2 = filtered_data.groupBy(\"weekday\", \"hour-period\", \"item\") \\\n",
    "                                   .agg(F.sum(\"Transaction\").alias(\"quantity\"))\n",
    "\n",
    "# Use window function to get the highest selling item per hour\n",
    "window_spec_2 = Window.partitionBy(\"weekday\",\"hour-period\").orderBy(F.desc(\"quantity\"))\n",
    "ranked_data_2 = grouped_bakeryData1_2.withColumn(\"rank\", F.row_number().over(window_spec_2))\n",
    "\n",
    "# Filter to get only the highest-selling item per hour and order the results by Date and Hour\n",
    "top_selling_items_2 = ranked_data_2.filter(F.col(\"rank\") == 1).orderBy(\"hour-period\")\n",
    "\n",
    "# Select the required cols and show results\n",
    "print(\"Show the highest selling item for Mondays, per hour, for the 7AM to 11AM hours. Note that 'weekday', 'period' have to be computed.\")\n",
    "top_selling_items_2.select(\"item\", \"quantity\", \"weekday\", \"hour-period\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc33d2-4540-442a-85d1-954bc7bf4993",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30885a39-7507-4826-8e53-bba0bb370240",
   "metadata": {},
   "source": [
    "#### Show the top 2 (by qty) items bought by Daypart, by DayType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59296a1e-97a9-45fe-ab3e-be52c230938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------+\n",
      "|DayType|  DayPart|           top2|\n",
      "+-------+---------+---------------+\n",
      "|Weekday|Breakfast|[Coffee, Bread]|\n",
      "|Weekday|   Dinner|[Coffee, Bread]|\n",
      "|Weekday|    Lunch|[Coffee, Bread]|\n",
      "|Weekend|Breakfast|[Coffee, Bread]|\n",
      "|Weekend|   Dinner|[Coffee, Bread]|\n",
      "|Weekend|    Lunch|[Coffee, Bread]|\n",
      "+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Weekday column based on the Date value\n",
    "# Add Hour-period column based on the Time value\n",
    "# Change transaction data type to Int.\n",
    "bakeryData2 = bakeryData.withColumn(\"Weekday\", F.date_format(\"date\", \"EEEE\")) \\\n",
    "                        .withColumn(\"Hour-period\", F.date_format(\"Time\", \"h\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"Transaction\", bakeryData[\"Transaction\"].cast(IntegerType()))\n",
    "\n",
    "# Add DayType by categorizing Weekday into Weekday or Weekends\n",
    "bakeryData2 = bakeryData2.withColumn(\"DayType\", \n",
    "                                     F.when(F.col(\"Weekday\").isin(\"Saturday\", \"Sunday\"), \"Weekend\")\\\n",
    "                                     .otherwise(\"Weekday\"))\n",
    "\n",
    "# Add DayPart by categorizing Hour-period in to Breakfast, Lunch, and Dinner\n",
    "bakeryData2 = bakeryData2.withColumn(\"DayPart\", \n",
    "                                     F.when((F.col(\"Hour-period\").between(6, 10)), \"Breakfast\")\\\n",
    "                                     .when((F.col(\"Hour-period\").between(11, 15)), \"Lunch\")\\\n",
    "                                     .otherwise(\"Dinner\"))\n",
    "\n",
    "# Group by same DayType, DayPart, Item and aggregate transaction as total sum \n",
    "grouped_bakeryData2 = bakeryData2.groupBy(\"DayType\", \"DayPart\", \"Item\")\\\n",
    "                                    .agg(F.sum(\"Transaction\").alias(\"quantity\"))\n",
    "\n",
    "# Create a window specification to order items within each DayType and Daypart\n",
    "window_spec_q2 = Window.partitionBy(\"DayType\", \"DayPart\").orderBy(F.desc(\"quantity\"))\n",
    "\n",
    "# Give row number for ranking \n",
    "ranked_bakeryData_q2 = grouped_bakeryData2.withColumn(\"rank\", F.row_number().over(window_spec_q2))\n",
    "\n",
    "# Filter only top 2 items in the ranking\n",
    "top2_items = ranked_bakeryData_q2.filter(F.col(\"rank\") <= 2)\n",
    "\n",
    "# Put top 2 items into a single column\n",
    "grouped_top2_items = top2_items.groupBy(\"DayType\", \"DayPart\")\\\n",
    "                                .agg(F.collect_list(F.col(\"Item\")).alias(\"top2\"))\n",
    "# Show result\n",
    "grouped_top2_items.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3997fb-8a0a-49ad-be7f-9302696e2ee0",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a897ca-4840-4ad9-965f-d4eec019e9a7",
   "metadata": {},
   "source": [
    "#### Show the number of entities by “fields.rpt_area_desc”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f62ef4d-f949-43b9-8d4a-5fe591a15f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|rpt_area_desc        |count|\n",
      "+---------------------+-----+\n",
      "|Bed&Breakfast Home   |4    |\n",
      "|Summer Camps         |4    |\n",
      "|Institutions         |30   |\n",
      "|Local Confinement    |2    |\n",
      "|Mobile Food          |147  |\n",
      "|School Buildings     |89   |\n",
      "|Summer Food          |242  |\n",
      "|Swimming Pools       |420  |\n",
      "|Day Care             |173  |\n",
      "|Tattoo Establishments|36   |\n",
      "|Residential Care     |154  |\n",
      "|Bed&Breakfast Inn    |2    |\n",
      "|Adult Day Care       |5    |\n",
      "|Lodging              |62   |\n",
      "|Food Service         |1093 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read json data\n",
    "restaurantData = spark.read.json(\"./data/Restaurants_in_Durham_County_NC.json\") \n",
    "\n",
    "# Group data by fields.rpt_area_desc and get the total count\n",
    "restaurantData_entity = restaurantData.groupBy(F.col(\"fields.rpt_area_desc\"))\\\n",
    "                                        .agg(F.count(\"*\").alias(\"count\"))\n",
    "# Show result\n",
    "restaurantData_entity.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd3597-d359-45c0-94d8-c8af252b9208",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271224a2-32d9-48e0-9d1b-23b171f41d76",
   "metadata": {},
   "source": [
    "#### Show the country or region with the biggest percentage increase in population AND the country with biggest percentage decrease in population, between the years 1990 and 2000. Use only the countries, not ‘World’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02fde670-2a86-4b3f-a9c1-ae9ef08976f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|             Country|         Change_%|\n",
      "+--------------------+-----------------+\n",
      "|United Arab Emirates|76.27926665641841|\n",
      "|          Montserrat|-63.1873277639145|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 00:26:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1990, 2000\n",
      " Schema: _c0, 1990, 2000\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/am12180/am12180-hw2/data/populationbycountry19802010millions.csv\n",
      "24/10/26 00:26:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1990, 2000\n",
      " Schema: _c0, 1990, 2000\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/am12180/am12180-hw2/data/populationbycountry19802010millions.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the csv data\n",
    "populationData = spark\\\n",
    "  .read\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"./data/populationbycountry19802010millions.csv\") \n",
    "\n",
    "# Rename the first column (unnamed) to Country\n",
    "# Change the population (millions) of 1990 and 2000 to float\n",
    "populationData = populationData.withColumnRenamed(\"_c0\", \"Country\")\\\n",
    "                                .withColumn(\"1990\", F.col(\"1990\").cast(\"float\"))\\\n",
    "                                .withColumn(\"2000\", F.col(\"2000\").cast(\"float\"))\\\n",
    "                                \n",
    "# Filter out the row with World population\n",
    "# Drop rows where either '1990' or '2000' columns are null (not numeric e.g. NULL NA or --)\n",
    "cleaned_populationData = populationData.filter(F.col(\"Country\") != \"World\")\\\n",
    "                                        .na.drop(subset=[\"1990\", \"2000\"])\n",
    "\n",
    "# Add a column with the value of the percentage change between 1990 and 2000\n",
    "growth_rate = cleaned_populationData.withColumn(\"Change_%\", (F.col(\"2000\") - F.col(\"1990\")) / F.col(\"1990\") * 100)\n",
    "\n",
    "# Order by change % in descending order to find max\n",
    "# Order by change % in ascending order to find min\n",
    "max_growth = growth_rate.select(\"Country\",\"Change_%\").orderBy(F.desc(\"Change_%\")).limit(1)\n",
    "min_growth = growth_rate.select(\"Country\",\"Change_%\").orderBy(F.asc(\"Change_%\")).limit(1)\n",
    "\n",
    "# Show both results as one table\n",
    "(max_growth.union(min_growth)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b55315-2b06-4902-8d48-f2f71a5f8d90",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05ad921d-e221-41a2-9eb0-578ea856c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "| the|163547|\n",
      "|  to| 89046|\n",
      "|   p| 78664|\n",
      "|  of| 75568|\n",
      "| and| 72730|\n",
      "|  in| 56782|\n",
      "|   a| 53198|\n",
      "| for| 29770|\n",
      "|that| 28852|\n",
      "|  is| 27601|\n",
      "+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import Tokenizer, NGram\n",
    "\n",
    "hw1Data = spark.read.text(\"./data/hw1text/*.txt\")\n",
    "\n",
    "# Set up a tokenizer to split each row of texts into words\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"value\", outputCol=\"words\",\n",
    "                                 pattern=\"\\\\W\",  # Split by non-word characters (punctuation, spaces, etc.)\n",
    "                                 toLowercase=True ) # Convert text to lowercase\n",
    "words = regex_tokenizer.transform(hw1Data)\n",
    "\n",
    "# Create unigram transformer and apply it to words\n",
    "unigram = NGram(n=1, inputCol=\"words\", outputCol=\"unigram\")\n",
    "unigram_data = unigram.transform(words)\n",
    "\n",
    "# Use explode function to transform the unigrams column to rows\n",
    "unigram_exploded = unigram_data.withColumn(\"word\", F.explode(F.col(\"unigram\")))\n",
    "\n",
    "# Group by word for total counts\n",
    "unigram_count = unigram_exploded.groupBy(\"word\").count().orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# Show result of top 10 counts\n",
    "unigram_count.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d81ed2-17c0-485b-b91d-d8dc36992c91",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b8434ef-2b0a-4711-b0e1-90acf94565b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  bigram|count|\n",
      "+--------+-----+\n",
      "|  of the|17484|\n",
      "|  in the|12808|\n",
      "|   p the|10363|\n",
      "|covid 19| 8762|\n",
      "|  to the| 8372|\n",
      "| for the| 5588|\n",
      "|     n t| 5393|\n",
      "|  on the| 5032|\n",
      "|   to be| 4581|\n",
      "| will be| 4177|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create bigram transformer and apply it to words\n",
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigram_data = bigram.transform(words)\n",
    "\n",
    "# Use explode function to transform the bigram column to rows\n",
    "bigram_exploded = bigram_data.withColumn(\"bigram\", F.explode(F.col(\"bigrams\")))\n",
    "\n",
    "# Group by word for total counts\n",
    "bigram_count = bigram_exploded.groupBy(\"bigram\").count().orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# Show result of top 10 counts\n",
    "bigram_count.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818cb27f-9353-4419-8fc1-ed408381baa8",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3967b1-6811-41a4-87a4-f4cbd5b0ec67",
   "metadata": {},
   "source": [
    "#### a) Find food service and active restaurants (“status” = “ACTIVE” and “\"rpt_area_desc\" = \"Food Service”) closest to the following coordinate: of 35.994914, -78.897133, and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de70a09-67a4-4169-ab50-5cef730a9c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-------------+-------------------------+-------------------+\n",
      "|premise_name            |status|rpt_area_desc|coordinates              |distance           |\n",
      "+------------------------+------+-------------+-------------------------+-------------------+\n",
      "|OLD HAVANA SANDWICH SHOP|ACTIVE|Food Service |[-78.8981331, 35.9932826]|0.12582219684232088|\n",
      "+------------------------+------+-------------+-------------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use haversine lib to calculate distance\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "# Given coordinates\n",
    "coords = (35.994914, -78.897133)\n",
    "\n",
    "# Distance function that takes in coordinates,\n",
    "# and calculates the distance to the coords given\n",
    "def distance(coordinates):\n",
    "    lat = coordinates[1]\n",
    "    long = coordinates[0]\n",
    "    return haversine(coords, (lat, long), unit=Unit.MILES)\n",
    "\n",
    "# Set distance function as a UDF\n",
    "distance_udf = F.udf(distance, DoubleType())\n",
    "\n",
    "# Filter only Active Food Services with Coordinates\n",
    "active_restaurant = restaurantData.filter(\n",
    "                    (F.col(\"fields.status\") == \"ACTIVE\") & \n",
    "                    (F.col(\"fields.rpt_area_desc\") == \"Food Service\") &\n",
    "                    (F.col(\"geometry.coordinates\").isNotNull()))\n",
    "\n",
    "# Add distance column with its value calculated from udf\n",
    "# Order by closest - farthest\n",
    "restaurant_with_distance = active_restaurant.withColumn(\"distance\", distance_udf(F.col(\"geometry.coordinates\")))                     \n",
    "\n",
    "# Show closest restaurant result\n",
    "closest_restaurant = restaurant_with_distance.orderBy(F.col(\"distance\"))\\\n",
    "                            .select(\"fields.premise_name\", \"fields.status\", \n",
    "                                    \"fields.rpt_area_desc\",\"geometry.coordinates\", \n",
    "                                    \"distance\")\\\n",
    "                            .limit(1)\n",
    "closest_restaurant.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbc1af-5fc3-4251-9ffd-65af3d9dfdbf",
   "metadata": {},
   "source": [
    "#### b) With that restaurant in (a) as your center point, find the number of foreclosures within a 1 mile radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfcae27-d8a4-4428-b296-7b1d70cdc1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coords of closest restaurant: (-78.8981331, 35.9932826)\n",
      "Number of foreclosures within 1 mile radius: 283\n",
      "+-------------+--------------------+--------------------+-------------------+\n",
      "|parcel_number|             address|         coordinates|           distance|\n",
      "+-------------+--------------------+--------------------+-------------------+\n",
      "|       110138|217 E CORPORATION ST|[-78.8922549, 36....|0.42018153731654434|\n",
      "|       110535|      401 N QUEEN ST|[-78.895396, 35.9...| 0.1920521426225851|\n",
      "|       110536|      403 N QUEEN ST|[-78.8950321, 35....| 0.2161257772521711|\n",
      "|       111324|      918 GILBERT ST|[-78.8873774, 35....| 0.7438842496954408|\n",
      "|       111399|      721 LIBERTY ST|[-78.888343, 35.9...| 0.6764401088260837|\n",
      "|       111426|      729 HOPKINS ST|[-78.888092, 35.9...| 0.6939319675911833|\n",
      "|       112166|      1302 E MAIN ST|[-78.886681, 35.9...| 0.7962788643952043|\n",
      "|       115974|       209 NELSON ST|[-78.9041979, 35....| 0.4984384851724365|\n",
      "|       116301|    2721 ATLANTIC ST|[-78.9060606, 35....| 0.6521640134298656|\n",
      "|       116319|     518 RED OAK AVE|[-78.903483, 35.9...| 0.5219934997146012|\n",
      "|       117172|          ROXBORO ST|[-78.8960511, 36....| 0.5966188975820941|\n",
      "|       117678|      1420 WABASH ST|[-78.890249, 35.9...| 0.6119955559085681|\n",
      "|       117926|       500 POTTER ST|[-78.889404, 35.9...| 0.6360404283316798|\n",
      "|       119062|       418 SOWELL ST|[-78.8873413, 35....| 0.7652072262852906|\n",
      "|       119143|       426 POTTER ST|[-78.8897521, 35....| 0.6033271855286035|\n",
      "|       119161|       708 COLFAX ST|[-78.893367, 35.9...|0.36216085379743657|\n",
      "|       119303|    323 E UMSTEAD ST|[-78.8997693, 35....|0.17693875152052202|\n",
      "|       119358|     21 POINCIANA DR|[-78.901759, 35.9...|0.27111869408861955|\n",
      "|       128338|       2820 BROAD ST|[-78.907898, 36.0...| 0.9051744303274862|\n",
      "|       132766|    1812 SHERMAN AVE|[-78.8897549, 35....| 0.7004869611552414|\n",
      "+-------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read foreclosure data\n",
    "foreclosures = spark.read.json(\"./data/durham-nc-foreclosure-2006-2016.json\") \n",
    "# Closest restaurant's coordinates from 7(a)\n",
    "a_coords = tuple(closest_restaurant.collect()[0].coordinates)\n",
    "\n",
    "# Distance function that takes in coordinates,\n",
    "# and calculates the distance to the closest restaurant's coords\n",
    "def b_distance(coordinates):\n",
    "    lat = coordinates[0]\n",
    "    long = coordinates[1]\n",
    "    return haversine(a_coords, (lat, long), unit=Unit.MILES)\n",
    "b_udf = F.udf(b_distance, DoubleType())\n",
    "\n",
    "# Filter out rows with no coordinates\n",
    "foreclosures_filtered = foreclosures.filter(F.col(\"geometry.coordinates\").isNotNull())\n",
    "\n",
    "# Add distance column and filter <= 1 mile\n",
    "foreclosure_with_distance = foreclosures_filtered.withColumn(\"distance\", b_udf(F.col(\"geometry.coordinates\")))\\\n",
    "                                                    .filter(F.col(\"distance\") <= 1)\n",
    "\n",
    "# Count the number of distinct foreclosures within the radius\n",
    "number_of_foreclosures = foreclosure_with_distance.select(\"fields.parcel_number\",\"fields.address\",\"distance\")\\\n",
    "                                                    .distinct().count()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Coords of closest restaurant: {a_coords}\")\n",
    "print(f\"Number of foreclosures within 1 mile radius: {number_of_foreclosures}\")\n",
    "foreclosure_with_distance.select(\"fields.parcel_number\",\"fields.address\", \"geometry.coordinates\", \"distance\").show(truncate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
