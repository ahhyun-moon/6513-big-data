{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537cc9d0-ba97-4334-9401-ca1acab2a387",
   "metadata": {},
   "source": [
    "Midterm Q2~4 - Ahhyun Moon (am12180@nyu.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ca3fb9-d9a0-4fb5-882e-c54533e6174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 14:19:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/conda/envs/bigdata-spark/lib/python3.11/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7b90f41fa350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"<my-app-name>\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7e71e8-e378-4e03-a7be-0c150eeaac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functinos from Pyspark SQL Library\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType, ArrayType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer, NGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf222e62-5d5d-4d8d-9f25-3dcfba7e68c7",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44816e1e-b4e8-4e7b-ab13-79431476df41",
   "metadata": {},
   "source": [
    "## Used same logic from HW2 to get bigram counts from HW1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f8309c-31a4-4ab5-bf4a-1ab586f7c09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 06:18:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| bigram|count|\n",
      "+-------+-----+\n",
      "| of the|17217|\n",
      "| in the|12045|\n",
      "|   p he|10876|\n",
      "| to the| 8227|\n",
      "|    n t| 5368|\n",
      "|for the| 5328|\n",
      "| on the| 4806|\n",
      "|  to be| 4522|\n",
      "|will be| 4171|\n",
      "|and the| 3881|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = spark.read.text(\"../am12180-hw2/data/hw1text/*.txt\")\\\n",
    "    .withColumn(\"file\", F.lcase(F.regexp_replace(\"value\", r\"[^0-9a-z]\", \" \")))\\\n",
    "    .withColumn(\"text\", F.regexp_replace(\"file\", r\"  +\", \" \"))\n",
    "\n",
    "# corpus.limit(5).show()\n",
    "tokenizer = RegexTokenizer(outputCol=\"words\", inputCol=\"file\", pattern=r\" +\")\n",
    "\n",
    "bigram = NGram(n=2)\n",
    "bigram.setInputCol(\"words\")\n",
    "bigram.setOutputCol(\"bigrams\")\n",
    "\n",
    "bigram_count = bigram\\\n",
    "                    .transform(tokenizer.transform(corpus))\\\n",
    "                    .select(F.explode(\"bigrams\").alias(\"bigram\"))\\\n",
    "                    .where(F.length(\"bigram\") > 0)\\\n",
    "                    .groupBy(\"bigram\").count()\\\n",
    "                    .orderBy(F.desc(\"count\"))\n",
    "bigram_count.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23ac26-f4b7-419a-a444-4f2298d9e9cc",
   "metadata": {},
   "source": [
    "## Get top 10 trigram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ce75b7-6aca-485c-af84-5d8de5d4f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           trigram|count|\n",
      "+------------------+-----+\n",
      "|           lt p gt| 1929|\n",
      "|            do n t| 1450|\n",
      "|     the spread of| 1182|\n",
      "|           p gt lt| 1037|\n",
      "|           gt lt p| 1023|\n",
      "|of the coronavirus|  862|\n",
      "|        as well as|  830|\n",
      "|     the number of|  821|\n",
      "|        one of the|  811|\n",
      "|     spread of the|  779|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trigram = NGram(n=3)\n",
    "trigram.setInputCol(\"words\")\n",
    "trigram.setOutputCol(\"trigrams\")\n",
    "\n",
    "trigram_count = trigram\\\n",
    "                    .transform(tokenizer.transform(corpus))\\\n",
    "                    .select(F.explode(\"trigrams\").alias(\"trigram\"))\\\n",
    "                    .where(F.length(\"trigram\") > 0)\\\n",
    "                    .groupBy(\"trigram\").count()\\\n",
    "                    .orderBy(F.desc(\"count\"))\\\n",
    "                    .limit(10)\n",
    "trigram_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ca90e-210b-4808-9c21-f527f3aa6fec",
   "metadata": {},
   "source": [
    "## Compute conditional probability of the third word of each trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92804cc5-3091-4099-ba99-4b2f564e87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for Q2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------+------------+-----------+\n",
      "|           trigram|count|    bigram|bigram_count|probability|\n",
      "+------------------+-----+----------+------------+-----------+\n",
      "|           lt p gt| 1929|      lt p|        1931|    0.99896|\n",
      "|            do n t| 1450|      do n|        1451|    0.99931|\n",
      "|     the spread of| 1182|the spread|        1288|     0.9177|\n",
      "|           p gt lt| 1037|      p gt|        1941|    0.53426|\n",
      "|           gt lt p| 1023|     gt lt|        1804|    0.56707|\n",
      "|of the coronavirus|  862|    of the|       17217|    0.05007|\n",
      "|        as well as|  830|   as well|        1134|    0.73192|\n",
      "|     the number of|  821|the number|         869|    0.94476|\n",
      "|        one of the|  811|    one of|        1322|    0.61346|\n",
      "|     spread of the|  779| spread of|        1417|    0.54975|\n",
      "+------------------+-----+----------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract bigram from each trigram in trigram_count\n",
    "trigram_count = trigram_count.withColumn(\"bigram\",\\\n",
    "                                         F.concat_ws(\" \", F.split(F.col(\"trigram\"), \" \").getItem(0),\\\n",
    "                                                            F.split(F.col(\"trigram\"), \" \").getItem(1)))\n",
    "\n",
    "# Join trigram df with bigram df on the extracted bigram\n",
    "trigram_with_bigram_count = trigram_count.join(\n",
    "    bigram_count.select(F.col(\"bigram\").alias(\"bigram_word\"), F.col(\"count\").alias(\"bigram_count\")),\n",
    "    trigram_count[\"bigram\"] == F.col(\"bigram_word\"),\n",
    "    \"left\" )\n",
    "\n",
    "# Calculate the conditional probability for each trigram\n",
    "trigram_cond_prob = trigram_with_bigram_count.withColumn(\"probability\", F.round(F.col(\"count\") / F.col(\"bigram_count\"), 5))\n",
    "\n",
    "print(\"Result for Q2:\")\n",
    "\n",
    "# Show the result\n",
    "trigram_cond_prob.select(\"trigram\", \"count\", \"bigram\", \"bigram_count\", \"probability\")\\\n",
    "                 .orderBy(F.col(\"count\").desc())\\\n",
    "                 .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a87e42-eecd-4c26-b8ae-f4ae8acb211d",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf7ea5d-6090-4df5-955f-72750466a149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for Q3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|dayPart  |top3                   |\n",
      "+---------+-----------------------+\n",
      "|afternoon|[Coffee, Bread, Tea]   |\n",
      "|evening  |[Coffee, Bread, Tea]   |\n",
      "|morning  |[Coffee, Bread, Pastry]|\n",
      "|noon     |[Coffee, Bread, Tea]   |\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bakeryData = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"../am12180-hw2/data/Bakery.csv\") \n",
    "bakeryData2 = bakeryData.withColumn(\"hour-period\", F.date_format(\"Time\", \"HH\").cast(IntegerType()))\n",
    "\n",
    "# Add DayPart by categorizing Hour-period in to morning, afternoon, and evening\n",
    "bakeryData2 = bakeryData2.withColumn(\"dayPart\", \n",
    "                                     F.when(F.col(\"hour-period\").between(6, 10), \"morning\")\\\n",
    "                                     .when(F.col(\"hour-period\").between(11, 13), \"noon\")\\\n",
    "                                     .when(F.col(\"hour-period\").between(14, 16), \"afternoon\")\\\n",
    "                                     .when(F.col(\"hour-period\").between(17, 23), \"evening\")\\\n",
    "                                     .otherwise(\"evening\")) # 0 - 5 am\n",
    "\n",
    "# Group by same DayPart, Item and aggregate transaction as total sum \n",
    "grouped_bakeryData2 = bakeryData2.groupBy(\"dayPart\", \"Item\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Create a window specification to order items within each and Daypart\n",
    "window_spec = Window.partitionBy(\"dayPart\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Give row number for ranking \n",
    "ranked_bakeryData = grouped_bakeryData2.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "\n",
    "# Filter only top 3 items in the ranking\n",
    "top3_items = ranked_bakeryData.filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "# Put top 3 items into a single column\n",
    "grouped_top3_items = top3_items.groupBy(\"dayPart\").agg(F.collect_list(F.col(\"Item\")).alias(\"top3\"))\n",
    "\n",
    "print(\"Result for Q3:\")\n",
    "# Show result\n",
    "grouped_top3_items.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a41ee6-d4aa-4e10-8195-9a0c6600cd25",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eff1b1c-e74b-4977-971c-72b535c89fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02f8e91-23eb-4253-92f5-1176497e23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b621e3e8-d1bc-4acd-9c83-2e6ea5cb1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+--------------------+--------------------+--------------------+\n",
      "|             authors| category|      date|            headline|                link|   short_description|\n",
      "+--------------------+---------+----------+--------------------+--------------------+--------------------+\n",
      "|Carla K. Johnson, AP|U.S. NEWS|2022-09-23|Over 4 Million Am...|https://www.huffp...|Health experts sa...|\n",
      "|      Mary Papenfuss|U.S. NEWS|2022-09-23|American Airlines...|https://www.huffp...|He was subdued by...|\n",
      "|       Elyse Wanshel|   COMEDY|2022-09-23|23 Of The Funnies...|https://www.huffp...|\"Until you have a...|\n",
      "|    Caroline Bologna|PARENTING|2022-09-23|The Funniest Twee...|https://www.huffp...|\"Accidentally put...|\n",
      "|      Nina Golgowski|U.S. NEWS|2022-09-22|Woman Who Called ...|https://www.huffp...|Amy Cooper accuse...|\n",
      "+--------------------+---------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "209527"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huffpost_df = spark.read.json(\"./Huffpost.json\")\n",
    "#  .sample(fraction=0.5, seed=3)\n",
    "huffpost_df.show(5)\n",
    "huffpost_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b6d43-60d0-43ee-8580-2a7cb9d1a365",
   "metadata": {},
   "source": [
    "## Find top 5 similar articles using Spark ML MinHshLSH Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec4cfa4-ffe8-402e-a16e-e3016ca9b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|   short_description| id|\n",
      "+--------------------+---+\n",
      "|White House offic...|  0|\n",
      "|The director told...|  1|\n",
      "|Residents of Miss...|  2|\n",
      "|The GOP is contin...|  3|\n",
      "|The lawsuit looks...|  4|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She\\u2019s Perfect\"\n",
    "query = spark.createDataFrame([(\"\", \"\", \"\", \"\", \"\", base)])\n",
    "df = huffpost_df.union(query)\n",
    "cleaned_df = df.select(\"short_description\").distinct().withColumn(\"id\", monotonically_increasing_id())\n",
    "print(cleaned_df.count())\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a11d1f-1888-4aed-b31f-0041c71b6378",
   "metadata": {},
   "source": [
    "### Tokenized the short description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361de66d-f9e0-4935-b6ec-91886095c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|   short_description| id|               words|\n",
      "+--------------------+---+--------------------+\n",
      "|White House offic...|  0|[white, house, of...|\n",
      "|The director told...|  1|[the, director, t...|\n",
      "|Residents of Miss...|  2|[residents, of, m...|\n",
      "|The GOP is contin...|  3|[the, gop, is, co...|\n",
      "|The lawsuit looks...|  4|[the, lawsuit, lo...|\n",
      "+--------------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = RegexTokenizer(pattern=\"\\\\W\", inputCol=\"short_description\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(cleaned_df).filter( F.size(F.col(\"words\")) != 0 )\n",
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71f526-c720-4761-9e50-fa64fdc4f2ce",
   "metadata": {},
   "source": [
    "### Mapped the short description tokens to sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334ec94e-c598-4ab7-9ca9-244512c8bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(2048, {131: 1.0, 251: 1.0, 271: 1.0, 344: 1.0, 583: 1.0, 611: 1.0, 779: 1.0, 812: 1.0, 952: 1.0, 999: 1.0, 1051: 1.0, 1238: 1.0, 1292: 1.0, 1444: 1.0, 1449: 1.0, 1626: 2.0, 1649: 1.0, 1681: 3.0, 1805: 1.0, 1843: 1.0, 1847: 1.0, 1876: 1.0, 1889: 1.0, 1999: 2.0, 2045: 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HashingTF to create feature vectors\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2048)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.head().rawFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe8a7a-1b93-43a6-8dd6-c532ae365d08",
   "metadata": {},
   "source": [
    "### Compute the inverse document frequency weights based on the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cb3ecda-08be-4eb5-86dd-c666cb31236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(2048, {131: 3.1348, 251: 4.6341, 271: 3.4098, 344: 4.4232, 583: 3.612, 611: 0.8947, 779: 0.9645, 812: 4.0275, 952: 0.8382, 999: 0.7505, 1051: 3.4936, 1238: 3.7234, 1292: 2.8062, 1444: 2.6402, 1449: 4.444, 1626: 6.9949, 1649: 5.8594, 1681: 1.3485, 1805: 6.4889, 1843: 4.2229, 1847: 2.1811, 1876: 5.1804, 1889: 4.3451, 1999: 1.9004, 2045: 2.1874})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the IDF model and transform the original feature vectors\n",
    "idf = IDF(minDocFreq=3,inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.head().features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941101f0-bfdc-4d5d-9e5b-d89380136ead",
   "metadata": {},
   "source": [
    "### Use MinHashLSH to approximate the similarity distance between short descriptions (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0bb3d86-a34d-4525-b022-21a2d3ff12b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(short_description=\"White House officials say the crux of the president's visit to the U.N. this year will be a full-throated condemnation of Russia and its brutal war.\", id=0, words=['white', 'house', 'officials', 'say', 'the', 'crux', 'of', 'the', 'president', 's', 'visit', 'to', 'the', 'u', 'n', 'this', 'year', 'will', 'be', 'a', 'full', 'throated', 'condemnation', 'of', 'russia', 'and', 'its', 'brutal', 'war'], rawFeatures=SparseVector(2048, {131: 1.0, 251: 1.0, 271: 1.0, 344: 1.0, 583: 1.0, 611: 1.0, 779: 1.0, 812: 1.0, 952: 1.0, 999: 1.0, 1051: 1.0, 1238: 1.0, 1292: 1.0, 1444: 1.0, 1449: 1.0, 1626: 2.0, 1649: 1.0, 1681: 3.0, 1805: 1.0, 1843: 1.0, 1847: 1.0, 1876: 1.0, 1889: 1.0, 1999: 2.0, 2045: 1.0}), features=SparseVector(2048, {131: 3.1348, 251: 4.6341, 271: 3.4098, 344: 4.4232, 583: 3.612, 611: 0.8947, 779: 0.9645, 812: 4.0275, 952: 0.8382, 999: 0.7505, 1051: 3.4936, 1238: 3.7234, 1292: 2.8062, 1444: 2.6402, 1449: 4.444, 1626: 6.9949, 1649: 5.8594, 1681: 1.3485, 1805: 6.4889, 1843: 4.2229, 1847: 2.1811, 1876: 5.1804, 1889: 4.3451, 1999: 1.9004, 2045: 2.1874}), hashes=[DenseVector([69662265.0])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the MinHashLSH model to the feature vectors\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", seed=12345)\n",
    "model = mh.fit(rescaledData)\n",
    "# Transform the feature data to include hash values\n",
    "transformedData = model.transform(rescaledData)\n",
    "transformedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4c2d99-3889-4d57-8632-404feb6c27a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+---------------+\n",
      "|   short_description|        id|               words|         rawFeatures|            features|         hashes|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+---------------+\n",
      "|Kitten Born With ...|8590028459|[kitten, born, wi...|(2048,[34,381,386...|(2048,[34,381,386...|[[1.5298664E8]]|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "baseArticle = transformedData.filter(F.col(\"short_description\") == base)\n",
    "baseArticle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d00872ef-9ae6-4db6-bc7a-f39bd94abe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+------------------+----------+\n",
      "| A_id|       A_description|      B_id|       B_description|   JaccardDistance|   jaccard|\n",
      "+-----+--------------------+----------+--------------------+------------------+----------+\n",
      "|76604|What kinds of tho...|8590028459|Kitten Born With ...|            0.9375|    0.9375|\n",
      "|13888|This post first a...|8590028459|Kitten Born With ...|            0.9375|    0.9375|\n",
      "|42409|The former secret...|8590028459|Kitten Born With ...|0.8666666666666667| 0.8666667|\n",
      "|59504|Thomas Lane, J. K...|8590028459|Kitten Born With ...|0.9444444444444444| 0.9444444|\n",
      "|59732|He won't use Dona...|8590028459|Kitten Born With ...|0.8846153846153846|0.88461536|\n",
      "+-----+--------------------+----------+--------------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "similarNews = model.approxSimilarityJoin(transformedData, F.broadcast(baseArticle), 1, distCol=\"JaccardDistance\").select(\n",
    "        F.col(\"datasetA.id\").alias(\"A_id\"),\n",
    "        F.col(\"datasetA.short_description\").alias(\"A_description\"),\n",
    "        F.col(\"datasetB.id\").alias(\"B_id\"),\n",
    "        F.col(\"datasetB.short_description\").alias(\"B_description\"),\n",
    "        F.col(\"JaccardDistance\")\n",
    "    ) \\\n",
    "    .filter(\"A_id != B_id\")\\\n",
    "    .withColumn(\"jaccard\", F.col(\"JaccardDistance\").cast(FloatType()))\n",
    "\n",
    "similarNews.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b4687-1351-42cb-bc09-d9973483bf15",
   "metadata": {},
   "source": [
    "### Find URL link, headline, category, and short description of the 5 most similar items to the Item above (based on the \"short_description\" field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0abcaafa-8903-4085-94cd-a37016dd2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = similarNews.orderBy(\"jaccard\") \\\n",
    "        .select(\"A_id\", \"A_description\", \"jaccard\") \\\n",
    "        .limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bc03337-c022-417e-98e0-5407646ba8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: https://www.huffingtonpost.com/entry/short-haircut-makeover-video_us_5b9c5c46e4b03a1dcc7e1459\n",
      "Headline: How A Simple Short Haircut Can Make For A Dramatic Makeover (VIDEO)\n",
      "Category: STYLE & BEAUTY\n",
      "Short Description: She's sexy and she knows it!\n",
      "Jaccard Distance: 0.7647058963775635\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/cheryl-boone-isaacs-academy-president-statement-diversity_us_569e3587e4b00f3e9862c429\n",
      "Headline: Academy President Releases Official Statement On The Oscars' Lack Of Diversity\n",
      "Category: ENTERTAINMENT\n",
      "Short Description: Amid the backlash, Cheryl Boone Isaacs says that she's \"heartbroken and frustrated.\"\n",
      "Jaccard Distance: 0.782608687877655\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/camille-cosby-defamation-deposition_us_56869f2de4b06fa688826fb1\n",
      "Headline: Camille Cosby Ordered To Testify In Defamation Suit Against Bill Cosby\n",
      "Category: ENTERTAINMENT\n",
      "Short Description: She's scheduled to give a deposition Jan. 6.\n",
      "Jaccard Distance: 0.800000011920929\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.comhttp://pubx.co/wfVGIs\n",
      "Headline: Dad Turns Himself Into Real-Life ‘Elf on the Shelf’\n",
      "Category: WEIRD NEWS\n",
      "Short Description: A father in Moncton, New Brunswick, knows if his kids are naughty and nice because he's the Elf on the Shelf who is keeping\n",
      "Jaccard Distance: 0.8064516186714172\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/drake-goes-above-literally-and-beyond-with-his-latest-rihanna-pda_us_57c5bcf7e4b0cdfc5ac98321\n",
      "Headline: Drake Goes Above (Literally) And Beyond With His Latest Rihanna PDA\n",
      "Category: ENTERTAINMENT\n",
      "Short Description: He's in love, he's in love, and he doesn't care who knows it!\n",
      "Jaccard Distance: 0.8095238208770752\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = top5.join(huffpost_df, top5.A_description == huffpost_df.short_description, \"left\")  \n",
    "result_list = result.select(\"headline\", \"link\", \"category\", \"short_description\", \"jaccard\").orderBy(\"jaccard\").collect()\n",
    "\n",
    "for item in result_list:\n",
    "    print(\"Link:\", item.link)\n",
    "    print(\"Headline:\", item.headline)\n",
    "    print(\"Category:\", item.category)\n",
    "    print(\"Short Description:\", item.short_description)\n",
    "    print(\"Jaccard Distance:\", item.jaccard)\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81168f65-ec5c-43cd-8500-8e86d6ecf4ce",
   "metadata": {},
   "source": [
    "## Comparison with Datasketch MinHash - Jaccard Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93f32d4e-7a51-4d62-9cd4-983691f4ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 165:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: https://www.huffingtonpost.com/entry/how-to-make-the-perfect-c_us_5b9dc24ee4b03a1dcc8c8503\n",
      "Headline: How to Make the Perfect Chocolate Chip Cookie\n",
      "Category: FOOD & DRINK\n",
      "Short Description: A cookie with the perfect combination of fat, flavor, and comfort. Who needs detox?\n",
      "Jaccard Similarity: 0.21875\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/chiles-rellenos-recipe_us_5b9d7fb8e4b03a1dcc88c9d3\n",
      "Headline: Recipe Of The Day: Chiles Rellenos\n",
      "Category: FOOD & DRINK\n",
      "Short Description: Topped with a tomato-and-avocado salsa.\n",
      "Jaccard Similarity: 0.203125\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/-westworld-trailer_us_5767f302e4b015db1bc9d30f\n",
      "Headline: The 'Westworld' Trailer Looks Like 'Jurassic Park' With Robots\n",
      "Category: ENTERTAINMENT\n",
      "Short Description: Life finds a way.\n",
      "Jaccard Similarity: 0.1796875\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/michael-caine-slams-young-actors-who-just-want-to-be-rich-and-famous_us_5732292be4b0bc9cb0484b71\n",
      "Headline: Michael Caine Slams Young Actors Who Just Want To Be 'Rich And Famous'\n",
      "Category: ENTERTAINMENT\n",
      "Short Description: \"They do a little part on television and everyone knows who they are. They can't really act.\"\n",
      "Jaccard Similarity: 0.1796875\n",
      "\n",
      "\n",
      "Link: https://www.huffingtonpost.com/entry/maryland-governor-candidate-breastfeeds-in-new-campaign-ad_us_5ab3a1dce4b0decad0475af7\n",
      "Headline: Maryland Governor Candidate Breastfeeds In New Campaign Ad\n",
      "Category: WOMEN\n",
      "Short Description: “I’m a mom. I’m a woman. And I want to be your next governor.”\n",
      "Jaccard Similarity: 0.171875\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Define base and compute base_minhash using datasketch's MinHash\n",
    "base = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect\"\n",
    "base_tokens = base.lower().split(' ')\n",
    "base_minhash = MinHash()\n",
    "# Update the base_minhash with each token\n",
    "for token in base_tokens:\n",
    "    base_minhash.update(token.encode('utf8'))\n",
    "\n",
    "# Set up a tokenizer to split each row of texts into words\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"short_description\", outputCol=\"splitted_description\",\n",
    "                                 pattern=\"\\\\W\",  # Split by non-word characters (punctuation, spaces, etc.)\n",
    "                                 toLowercase=True ) # Convert text to lowercase\n",
    "words = regex_tokenizer.transform(huffpost_df)\n",
    "\n",
    "huffpost_df_splitted = words.select(\"link\", \"headline\", \"category\", \"short_description\",\"splitted_description\")\n",
    "# huffpost_df_splitted.show()\n",
    "# Define UDF with the return type as FloatType\n",
    "@F.udf(FloatType())\n",
    "def minHashEncode(splitted_description):\n",
    "    # Create a MinHash for the current description\n",
    "    min_hash = MinHash()  \n",
    "    min_hash.update_batch([s.encode('utf-8') for s in splitted_description])\n",
    "    return base_minhash.jaccard(min_hash)\n",
    "\n",
    "# Add a column with the Jaccard similarity score\n",
    "df_with_jaccard = huffpost_df_splitted.withColumn(\"jaccard\", minHashEncode(F.col(\"splitted_description\")))\n",
    "df_jaccard_filtered = df_with_jaccard.filter(F.col(\"jaccard\") > 0)\n",
    "# df_jaccard_filtered.show()\n",
    "top_5 = df_jaccard_filtered.orderBy(F.desc(\"jaccard\")).take(5)\n",
    "for item in top_5:\n",
    "    print(\"Link:\", item.link)\n",
    "    print(\"Headline:\", item.headline)\n",
    "    print(\"Category:\", item.category)\n",
    "    print(\"Short Description:\", item.short_description)\n",
    "    print(\"Jaccard Similarity:\", item.jaccard)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc2144-ea66-4296-a58f-eb69037607eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
